---
title: "Forcing data"
author: "Beni"
date: "11/17/2021"
output: html_document
---

```{r setup, include=FALSE}
library(ingestr)
library(tidyverse)
library(knitr)
library(ingestr)
```

## Read sites

```{r warning=FALSE, message=FALSE}
df_sites <- read_csv(file = "../data/df_sites.csv")
```

## MODIS

```{r eval = FALSE}
filn <- "../data/df_modis_fpar.rds"
if (!file.exists(filn)){
  settings_modis <- get_settings_modis(
    bundle            = "modis_fpar",
    data_path         = "~/data/modis_subsets/",
    method_interpol   = "linear",    # is more robust than other methods against values below 0 and above 1
    network = c("fluxnet", "icos"),
    keep              = TRUE,
    overwrite_raw     = FALSE,
    overwrite_interpol= TRUE,
    n_focal           = 0
    )
  
  df_modis_fpar <- ingest(
    df_sites,
    source = "modis",
    settings = settings_modis,
    parallel = FALSE,
    ncores = 1
    )
  
  saveRDS(df_modis_fpar, file = filn)
    
} else {
  df_modis_fpar <- read_rds(filn)
}
```

Check data availability. Must not have any missing.
```{r}
visdat::vis_miss(df_modis_fpar %>% unnest(data), cluster = FALSE, warn_large_data = FALSE)
```

Determine MODIS-era for each site based on years for which data from all days are available.
```{r}
## determine year_start as firs year for which all data is available
get_year_start <- function(df){
  years <- df %>% 
    mutate(year = lubridate::year(date),
           avl_fapar = !is.na(modisvar_filled)) %>% 
    group_by(year) %>% 
    summarise(avl_fapar = sum(avl_fapar)) %>% 
    dplyr::filter(avl_fapar >= 365) %>% 
    pull(year)
  if (length(years)==0){
    return(NA)
  } else {
    return(min(as.integer(years)))
  }
}
get_year_end <- function(df){
  years <- df %>% 
    mutate(year = lubridate::year(date),
           avl_fapar = !is.na(modisvar_filled)) %>% 
    group_by(year) %>% 
    summarise(avl_fapar = sum(avl_fapar)) %>% 
    dplyr::filter(avl_fapar >= 365) %>% 
    pull(year)
  if (length(years)==0){
    return(NA)
  } else {
    return(max(as.integer(years)))
  }
}

df_sites_modis <- df_modis_fpar %>% 
  mutate(year_start = purrr::map_int(data, ~get_year_start(.)),
         year_end = purrr::map_int(data, ~get_year_end(.))) %>% 
  dplyr::select(-data)
```

Update site meta info data based on available modis years. For some sites, no MODIS data covering at least one full year is available. Drop them. This leaves 45 sites.
```{r}
df_sites_modis_era <- df_sites %>% 
  dplyr::select(-year_start, -year_end) %>% 
  left_join(
    df_sites_modis,
    by = "sitename"
  ) %>% 
  drop_na(year_start, year_end)

saveRDS(df_sites_modis_era, file = "../data/df_sites_modis_era.csv")
```

Drop data for which site/year is not complete.
```{r}
df_modis_fpar <- df_modis_fpar %>% 
  left_join(df_sites_modis_era %>% 
              dplyr::select(sitename, year_start, year_end), 
            by = "sitename"
            ) %>% 
  unnest(data) %>% 
  mutate(year = lubridate::year(date)) %>% 
  mutate(keep = ifelse(year>= year_start & year <= year_end, TRUE, FALSE)) %>% 
  dplyr::filter(keep) %>% 
  dplyr::select(-keep) %>% 
  group_by(sitename) %>% 
  nest()

## old ingestr version
df_modis_fpar <- df_modis_fpar %>% 
  mutate(data = purrr::map(data, ~rename(., fapar = modisvar_filled)))

saveRDS(df_modis_fpar, file = "../data/df_modis_fpar_modis_era.csv")
```

Check data availability. Must not have any missing.
```{r}
visdat::vis_miss(df_modis_fpar %>% unnest(data), cluster = FALSE, warn_large_data = FALSE)
```

## FLUXNET data

```{r eval = FALSE}
filn <- "../data/df_fluxnet.rds"
if (!file.exists(filn)){
  df_fluxnet <-
    ingestr::ingest(
      siteinfo  = df_sites_modis_era,
      source    = "fluxnet",
      getvars   = list(
        temp = "TA_F_DAY",
        prec = "P_F",
        vpd  = "VPD_F_DAY",
        ppfd = "SW_IN_F",
        patm = "PA_F",
        tmin = "TMIN_F",
        tmax = "TMAX_F"),
      dir       = "~/data/FLUXNET-2015_Tier1/20191024/DD/",
      settings  = list(
        dir_hh = "~/data/FLUXNET-2015_Tier1/20191024/HH/", 
        dir_hr = "~/data/FLUXNET-2015_Tier1/20191024/HR/",
        getswc = FALSE
        ),
      timescale = "d"
        )
  
  saveRDS(df_fluxnet, file = filn)
    
} else {
  df_fluxnet <- read_rds(filn)
}
```

## CRU data

```{r eval = FALSE}
filn <- "../data/df_cru.rds"
if (!file.exists(filn)){
  df_cru <- ingestr::ingest(
    siteinfo  = df_sites_modis_era,
    source    = "cru",
    getvars   = "ccov",
    dir       = "~/data/cru/ts_4.01/"
    )
  
  saveRDS(df_cru, file = filn)
} else {
  df_cru <- read_rds(filn)
}
```

## Meteo data

Derived variables for meteo data frame.
```{r}
df_fluxnet <- df_fluxnet %>% 
  unnest(data) %>% 
  
  ## fill missing temp based on mean tmin and tmax
  mutate(temp = ifelse(is.na(temp), (tmin + tmax)/2, temp)) %>% 
  
  ## rain and snow
  mutate(rain = ifelse(temp > 1,  prec, 0),
         snow = ifelse(temp <= 1, prec, 0)) %>% 
  
  group_by(sitename) %>% 
  nest()
  
## combine with CRU data
df_meteo <- df_fluxnet %>%
  tidyr::unnest(data) %>%
  left_join(
    df_cru %>%
      tidyr::unnest(data),
    by = c("sitename", "date")
  ) %>%
  
  ## discard pre-modis era data
  mutate(year = lubridate::year(date)) %>% 
  dplyr::filter(year >= 2000) %>% 
  
  group_by(sitename) %>%
  tidyr::nest()
```

Check data availability. Must not have any missing. Ok.
```{r}
visdat::vis_miss(df_meteo %>% unnest(data), cluster = FALSE, warn_large_data = FALSE)
```

## CO2

```{r eval = FALSE}
# . grab CO2 data ----
df_co2 <- ingestr::ingest(
  df_sites_modis_era,
  source  = "co2_mlo",
  verbose = FALSE
  )
```

## Model-related stuff

A final set of required ancillary data covers the soil texture properties.

```{r eval = FALSE}
# . set soil parameters ----
df_soiltexture <- bind_rows(
  top    = tibble(
    layer = "top",
    fsand = 0.4,
    fclay = 0.3,
    forg = 0.1,
    fgravel = 0.1
    ),
  bottom = tibble(
    layer = "bottom",
    fsand = 0.4,
    fclay = 0.3,
    forg = 0.1,
    fgravel = 0.1)
)
```

## Simulation parameters

With all data downloads done we have to set some simulation parameters which
will be merged with all driver data. This data includes the number of spin-up
years, and how soil and temperature stress are dealt with.

```{r eval = FALSE}
params_siml <- list(
  spinup             = TRUE,
  spinupyears        = 10,
  recycle            = 1,
  soilmstress        = TRUE,
  tempstress         = TRUE,
  calc_aet_fapar_vpd = FALSE,
  in_ppfd            = TRUE,
  in_netrad          = FALSE,
  outdt              = 1,
  ltre               = FALSE,
  ltne               = FALSE,
  ltrd               = FALSE,
  ltnd               = FALSE,
  lgr3               = TRUE,
  lgn3               = FALSE,
  lgr4               = FALSE
	)
```

With all pieces in place we can use the `rsofun` collect_drivers_sofun()
function to merge all datasets into one nested dataframe, which serves as
input to the `rsofun` p-model. This concludes the formatting of the driver data

```{r eval = FALSE}
p_model_fluxnet_drivers <- rsofun::collect_drivers_sofun(
  site_info      = df_sites_modis_era,
  params_siml    = params_siml,
  meteo          = df_meteo,
  fapar          = df_modis_fpar,
  co2            = df_co2,
  params_soil    = df_soiltexture
  )

saveRDS(p_model_fluxnet_drivers, file = "../data/p_model_fluxnet_drivers.rds")
```

## Model test run

```{r}
# run the model for these parameters
# optimized parameters from previous work (Stocker et al., 2020 GMD)
params_modl <- list(
    kphio           = 0.09423773,
    soilm_par_a     = 0.33349283,
    soilm_par_b     = 1.45602286,
    tau_acclim_tempstress = 10,
    par_shape_tempstress  = 0.0
  )

output <- rsofun::runread_pmodel_f(
  p_model_fluxnet_drivers,
  par = params_modl
  )

output$data[[1]] %>% 
  ggplot(aes(date, gpp)) +
  geom_line()
```
